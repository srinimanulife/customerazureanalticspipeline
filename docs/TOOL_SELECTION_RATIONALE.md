# Tool Selection Rationale - Azure Data Platform MVP

This document explains why each Azure service was selected for the Customer Analytics Platform, what alternatives were considered, and the trade-offs involved.

---

## Table of Contents
1. [Azure Active Directory Groups](#1-azure-active-directory-groups)
2. [Azure Storage Account](#2-azure-storage-account)
3. [Storage Containers](#3-storage-containers-bronzeservergold)
4. [Azure Databricks](#4-azure-databricks)
5. [Azure Synapse Analytics](#5-azure-synapse-analytics)
6. [Azure Data Factory](#6-azure-data-factory)
7. [Summary Comparison Table](#summary-comparison-table)

---

## 1. Azure Active Directory Groups

### What We Used
**Azure Active Directory (AAD) Security Groups** with Role-Based Access Control (RBAC)

### How It's Used in MVP
```
Three AAD Groups Created:
1. data-engineers        â†’ Full access (Contributor on all resources)
2. data-analysts         â†’ Read-only access (Reader on Gold layer only)
3. data-platform-admins  â†’ Infrastructure management (Owner on resource group)

Access Control:
- Storage Blob Data Contributor â†’ data-engineers
- Storage Blob Data Reader â†’ data-analysts (Gold only)
- Synapse Administrator â†’ data-engineers
- Synapse SQL User â†’ data-analysts
- Data Factory Contributor â†’ data-engineers
```

### Alternatives Considered

| Option | Description | Why NOT Chosen |
|--------|-------------|----------------|
| **Individual User Assignments** | Assign RBAC roles directly to users | âŒ Not scalable - hard to manage 100+ users<br>âŒ No group-based auditing<br>âŒ Difficult to onboard/offboard |
| **Azure AD Application Roles** | Custom roles defined in app manifest | âŒ Requires custom application<br>âŒ More complex setup<br>âŒ Overkill for infrastructure access |
| **SQL Authentication** | Username/password for databases | âŒ Not allowed in FedRAMP High<br>âŒ Password management overhead<br>âŒ No MFA support |
| **Shared Access Signatures (SAS)** | Token-based access to storage | âŒ Token expiration management<br>âŒ No user-level auditing<br>âŒ Security risk if leaked |
| **Azure AD Privileged Identity Management (PIM)** | Just-in-time admin access | âœ… Could be added for production<br>âŒ Adds complexity for MVP<br>âŒ Requires Premium P2 license |

### Why Azure AD Groups + RBAC Won

âœ… **Native Azure Integration**: Works seamlessly with all Azure services
âœ… **Scalability**: Add/remove users from groups, permissions update automatically
âœ… **Compliance**: Required for FedRAMP High (no SQL auth allowed)
âœ… **Audit Trail**: All access logged in Azure AD sign-in logs
âœ… **MFA Support**: Conditional access policies can enforce MFA
âœ… **Zero Trust**: Identity-based access, not network-based
âœ… **Least Privilege**: Different groups for different access levels
âœ… **Government Ready**: Fully supported in Azure Government Cloud

### Trade-offs Accepted

| Trade-off | Impact | Mitigation |
|-----------|--------|------------|
| Coarse-grained permissions | Analysts get ALL of Gold, can't restrict to specific tables | Use Synapse row-level security if needed |
| Group management overhead | IT team must maintain group memberships | Automate with Azure AD dynamic groups |
| No just-in-time access | Engineers always have access | Add PIM in production for admin roles |

### Interview Talking Point
> "I chose Azure AD Groups with RBAC because it's the native, scalable, and compliant way to manage access in Azure Government. Unlike SQL authentication or SAS tokens, AD groups provide centralized identity management, full audit trails, and support for MFA through Conditional Access policies. This is required for FedRAMP High compliance and follows the principle of least privilege with three distinct access levels."

---

## 2. Azure Storage Account

### What We Used
**Azure Data Lake Storage Gen2 (ADLS Gen2)** - StorageV2 with Hierarchical Namespace enabled

### How It's Used in MVP
```
Configuration:
- Account Kind: StorageV2
- Hierarchical Namespace: Enabled (Data Lake Gen2)
- Replication: LRS (dev), GRS (production)
- Performance Tier: Standard
- Access Tier: Hot
- Network: Private endpoints only (Gov Cloud requirement)

Features Used:
âœ“ Blob storage for files
âœ“ Hierarchical namespace for folder structures
âœ“ POSIX-compliant ACLs
âœ“ Integration with Databricks, Synapse, Data Factory
âœ“ Lifecycle management policies
```

### Alternatives Considered

| Option | Description | Why NOT Chosen |
|--------|-------------|----------------|
| **Azure Blob Storage (Standard)** | Basic blob storage without hierarchical namespace | âŒ Poor performance with big data frameworks<br>âŒ No POSIX ACLs<br>âŒ Slower directory operations<br>âŒ Not optimized for analytics |
| **Azure Files** | SMB file shares | âŒ Not designed for big data<br>âŒ Expensive for large datasets<br>âŒ Slower for analytics workloads<br>âŒ Limited Spark integration |
| **Azure NetApp Files** | Enterprise NFS file storage | âŒ Overkill for this use case<br>âŒ 10x more expensive<br>âŒ Designed for OLTP, not analytics<br>âŒ Minimum 4TB commitment |
| **SQL Database Storage** | Store data directly in SQL tables | âŒ Expensive at scale ($/GB much higher)<br>âŒ Not flexible for semi-structured data<br>âŒ Harder to process with Spark<br>âŒ Lock-in to SQL format |
| **Azure Cosmos DB** | NoSQL database | âŒ Overkill - don't need global distribution<br>âŒ More expensive<br>âŒ Designed for operational workloads<br>âŒ Adds unnecessary complexity |
| **HDFS on HDInsight** | Hadoop Distributed File System | âŒ Must manage cluster storage separately<br>âŒ More expensive (always-on cluster)<br>âŒ Less durable than blob storage<br>âŒ Legacy approach |

### Why ADLS Gen2 Won

âœ… **Optimized for Big Data**: Native integration with Spark, Hive, etc.
âœ… **Performance**: Hierarchical namespace enables fast directory operations
âœ… **Cost-Effective**: Pay only for storage used (~$18/TB/month)
âœ… **Scalability**: Petabyte-scale without limits
âœ… **Multi-Protocol**: Supports both Blob API and ABFS (Azure Blob File System)
âœ… **Security**: ACLs at file/folder level + RBAC
âœ… **Lifecycle Management**: Auto-tier to Cool/Archive for cost optimization
âœ… **Analytics Integration**: Works seamlessly with all Azure analytics services
âœ… **Durability**: 11 nines (99.999999999%) durability with ZRS/GRS

### ADLS Gen2 vs Regular Blob Storage

| Feature | Regular Blob | ADLS Gen2 | Impact |
|---------|--------------|-----------|--------|
| Hierarchical namespace | âŒ Flat | âœ… Yes | Gen2: 10x faster directory operations |
| POSIX ACLs | âŒ No | âœ… Yes | Gen2: Fine-grained security |
| Directory rename | âŒ Copy all blobs | âœ… Atomic | Gen2: Instant vs minutes |
| Spark performance | âš ï¸ Slower | âœ… Optimized | Gen2: 2-3x faster for analytics |
| Price | Same | Same | No cost difference |

### Trade-offs Accepted

| Trade-off | Impact | Mitigation |
|-----------|--------|------------|
| Gen2 cannot be converted back to regular blob | Committed to Gen2 | Not an issue - Gen2 is superset |
| Slightly more complex than regular blob | Learning curve | Benefits far outweigh complexity |
| Some blob features not available (e.g., blob indexing) | Limited search | Use external catalog (Azure Purview) |

### Interview Talking Point
> "I chose Azure Data Lake Storage Gen2 because it's purpose-built for big data analytics. Unlike regular blob storage, Gen2 has a hierarchical namespace that enables 10x faster directory operations and POSIX-compliant ACLs for fine-grained security. It's the same price as blob storage but optimized for Spark workloads, providing 2-3x better performance. With petabyte-scale capability and $18/TB/month pricing, it's the most cost-effective solution for analytics workloads in Azure."

---

## 3. Storage Containers (Bronze/Silver/Gold)

### What We Used
**Medallion Architecture** with three separate containers: Bronze â†’ Silver â†’ Gold

### How It's Used in MVP
```
bronze/
â”œâ”€â”€ transactions/     â†’ Raw CSV files as ingested
â”œâ”€â”€ customers/        â†’ Raw customer data
â””â”€â”€ .placeholder      â†’ Folder structure markers

silver/
â””â”€â”€ transactions/     â†’ Cleaned Parquet files
    â”œâ”€â”€ transaction_date=2025-01-15/
    â”œâ”€â”€ transaction_date=2025-01-16/
    â””â”€â”€ ...

gold/
â”œâ”€â”€ daily_sales_summary/          â†’ Delta tables
â”œâ”€â”€ customer_segmentation/        â†’ Aggregated metrics
â”œâ”€â”€ product_category_performance/
â”œâ”€â”€ store_performance/
â””â”€â”€ payment_method_analysis/

scripts/              â†’ Separate container for configs
```

### Alternatives Considered

| Option | Description | Why NOT Chosen |
|--------|-------------|----------------|
| **Single Container with Folders** | /raw, /processed, /curated in one container | âŒ Cannot set different access policies<br>âŒ No container-level monitoring<br>âŒ Harder to manage lifecycle<br>âŒ All-or-nothing RBAC |
| **Two-Layer (Raw + Curated)** | Skip the Silver layer | âŒ Lose intermediate validation checkpoint<br>âŒ Must reprocess from raw if gold fails<br>âŒ No separation between cleaning and aggregation<br>âŒ Harder to debug issues |
| **Four+ Layers** | Add Landing, Archive, Error zones | âš ï¸ Valid for complex cases<br>âŒ Overkill for MVP<br>âŒ More maintenance overhead<br>âœ… Could add later |
| **Separate Storage Accounts** | Different storage account per layer | âŒ More expensive (multiple accounts)<br>âŒ More complex networking<br>âŒ Unnecessary isolation<br>âœ… Used in multi-tenant scenarios |
| **Database Tables Instead** | Use SQL tables for each layer | âŒ Expensive at scale<br>âŒ Less flexible formats<br>âŒ Vendor lock-in<br>âŒ Harder to access raw data |

### Why Medallion (Bronze/Silver/Gold) Won

âœ… **Industry Standard**: Widely adopted pattern from Databricks
âœ… **Clear Separation of Concerns**: Each layer has a specific purpose
âœ… **Incremental Complexity**: Bronze (simple), Silver (cleaned), Gold (business-ready)
âœ… **Replay Capability**: Can reprocess Silver/Gold without re-ingesting Bronze
âœ… **Different Access Policies**: Analysts only see Gold, engineers see all
âœ… **Auditability**: Can trace data lineage through layers
âœ… **Format Flexibility**: CSV â†’ Parquet â†’ Delta (right tool for each stage)
âœ… **Debugging**: Can verify data quality at each checkpoint

### Layer-by-Layer Rationale

#### Bronze Layer (Raw)
**Purpose**: Store data exactly as received
**Format**: CSV (source format)
**Why**:
- âœ… Immutable audit trail
- âœ… Can replay if transformation logic changes
- âœ… Compliance requirement (keep original records)
- âœ… No data loss risk

#### Silver Layer (Cleaned)
**Purpose**: Validated, deduplicated, standardized data
**Format**: Parquet with Snappy compression
**Why**:
- âœ… Parquet: Columnar format, 10x smaller than CSV
- âœ… Schema enforcement (no corrupted data downstream)
- âœ… Partitioned by date for faster queries
- âœ… Checksum validation
- âœ… Type-safe (decimals, dates, not strings)

#### Gold Layer (Business-Ready)
**Purpose**: Pre-aggregated, optimized for BI tools
**Format**: Delta Lake tables
**Why**:
- âœ… Delta: ACID transactions (safe concurrent writes)
- âœ… Time travel (query historical versions)
- âœ… Schema evolution (add columns without breaking)
- âœ… Z-ordering for query performance
- âœ… Automatic compaction
- âœ… Direct SQL access via Synapse

### Bronze/Silver/Gold Decision Matrix

| Requirement | Single Container | 2-Layer | 3-Layer (Medallion) âœ… | 4-Layer |
|-------------|------------------|---------|------------------------|---------|
| RBAC separation | âŒ No | âš ï¸ Limited | âœ… Full control | âœ… Full control |
| Replay data | âš ï¸ Hard | âœ… Yes | âœ… Yes | âœ… Yes |
| Debug intermediate | âŒ No | âš ï¸ Limited | âœ… Easy | âœ… Very easy |
| Complexity | âœ… Simple | âœ… Simple | âš ï¸ Medium | âŒ High |
| Cost | âœ… Lowest | âœ… Low | âš ï¸ Medium | âŒ Higher |
| Compliance friendly | âŒ No | âš ï¸ OK | âœ… Yes | âœ… Yes |

### Trade-offs Accepted

| Trade-off | Impact | Mitigation |
|-----------|--------|------------|
| 3x storage (same data in 3 layers) | Higher storage cost | Lifecycle policies move old data to Cool tier |
| More complex pipeline | More code to maintain | Clear separation makes debugging easier |
| Longer end-to-end latency | Bronze â†’ Silver â†’ Gold takes time | For real-time, add streaming path directly to Gold |

### Interview Talking Point
> "I implemented the Medallion architectureâ€”Bronze, Silver, Goldâ€”because it's the industry standard for data lakes. Bronze stores raw data exactly as received for audit and replay capability. Silver contains cleaned, validated Parquet files that are 10x smaller than CSV and partitioned for performance. Gold uses Delta Lake for ACID transactions and provides pre-aggregated tables optimized for analysts. This separation allows different access policiesâ€”analysts only see Gold, while engineers can debug at each layer. The storage cost of keeping all three layers is offset by the operational benefits: faster debugging, ability to reprocess without re-ingestion, and compliance with data retention requirements."

---

## 4. Azure Databricks

### What We Used
**Azure Databricks Premium** with auto-scaling clusters

### How It's Used in MVP
```
Workspace Configuration:
- SKU: Premium (required for RBAC, Gov Cloud)
- Cluster: Auto-scaling (1-4 workers, Standard_DS3_v2)
- Auto-termination: 20 minutes of inactivity
- Runtime: 13.3.x LTS with Scala 2.12

Two Notebooks:
1. bronze_to_silver.py
   - Reads CSV from Bronze
   - Schema validation
   - Data quality checks
   - Writes Parquet to Silver

2. silver_to_gold.py
   - Reads Parquet from Silver
   - Business aggregations
   - Customer segmentation
   - Writes Delta to Gold

Libraries:
- PySpark (built-in)
- pandas
- azure-storage-blob
```

### Alternatives Considered

| Option | Description | Why NOT Chosen |
|--------|-------------|----------------|
| **Azure Synapse Spark Pools** | Spark in Synapse Analytics | âš ï¸ **Could work**, but:<br>âŒ Slower startup (2-3 min cold start)<br>âŒ Less mature than Databricks<br>âŒ Fewer optimizations<br>âœ… Better if already using Synapse heavily |
| **HDInsight Spark** | Managed Hadoop/Spark clusters | âŒ Legacy service (maintenance mode)<br>âŒ Must manage cluster yourself<br>âŒ Slower innovation<br>âŒ Microsoft recommends Databricks instead |
| **Data Factory Data Flows** | Visual data transformation in ADF | âŒ Limited for complex logic<br>âŒ Less flexible than code<br>âŒ Debugging is harder<br>âœ… OK for simple transformations |
| **Azure Functions + Python** | Serverless processing | âŒ Not designed for big data<br>âŒ 10-minute execution limit<br>âŒ Memory constraints<br>âŒ No distributed processing<br>âœ… Good for small files |
| **Azure Batch** | Batch job processing | âŒ Must write all distributed logic yourself<br>âŒ No Spark/analytics framework<br>âŒ More DevOps overhead<br>âœ… Good for HPC workloads |
| **Synapse Dedicated SQL Pool** | T-SQL transformations | âŒ Expensive for large transformations<br>âŒ Less flexible than Spark<br>âŒ Not good for semi-structured data<br>âœ… Good for SQL-heavy workloads |
| **On-Premises Spark** | Self-managed Spark on VMs | âŒ Must manage infrastructure<br>âŒ No auto-scaling<br>âŒ Higher operational burden<br>âŒ More expensive |

### Why Databricks Won

âœ… **Best-in-Class Spark**: Created by original Spark developers
âœ… **Performance**: Photon engine 3-4x faster than open-source Spark
âœ… **Auto-Scaling**: Scale from 1 to 100s of workers automatically
âœ… **Cost Optimization**: Auto-pause after 20 min, pay per second
âœ… **Enterprise Features**: RBAC, audit logs, secret management
âœ… **Notebook Experience**: Interactive development, visualizations
âœ… **Delta Lake**: Native support for ACID transactions
âœ… **Azure Integration**: Managed identity, private endpoints
âœ… **Gov Cloud Support**: Available in US Gov Virginia
âœ… **Developer Productivity**: Much faster than writing raw Spark jobs

### Databricks vs Synapse Spark Comparison

| Feature | Databricks | Synapse Spark | Winner |
|---------|------------|---------------|--------|
| **Startup time** | 2-3 minutes | 3-5 minutes | Databricks |
| **Auto-scaling** | Very smooth | Good | Databricks |
| **Notebook experience** | Excellent | Good | Databricks |
| **Performance (Photon)** | 3-4x faster | Standard | Databricks |
| **Delta Lake** | Native | Supported | Databricks |
| **ML capabilities** | MLflow built-in | Limited | Databricks |
| **Integration with Synapse SQL** | Via connector | Native | Synapse |
| **Pricing** | DBU + compute | Compute only | Synapse (slightly cheaper) |
| **Maturity** | Very mature | Newer | Databricks |
| **Gov Cloud availability** | âœ… Yes | âœ… Yes | Tie |

### When to Choose Synapse Spark Instead
âœ… You're heavily invested in Synapse SQL pools
âœ… Budget is extremely tight (no DBU cost)
âœ… Simple transformations (not worth Databricks premium)
âœ… Need tight integration with Synapse pipelines

### When Databricks is Better (Our Case)
âœ… Complex data transformations âœ“
âœ… Need best performance âœ“
âœ… Interactive development important âœ“
âœ… Plan to use ML in future âœ“
âœ… Want industry-standard platform âœ“

### Trade-offs Accepted

| Trade-off | Impact | Mitigation |
|-----------|--------|------------|
| DBU cost on top of compute | ~40% more expensive than Synapse Spark | Auto-terminate saves 60-70% vs always-on |
| Another service to learn | Learning curve | Databricks has excellent documentation |
| Vendor lock-in to Databricks | Harder to switch | Use open formats (Delta, Parquet) |

### Cost Comparison (Monthly for Dev Environment)

| Option | Cost | Notes |
|--------|------|-------|
| **Databricks** (MVP) | **$150-200** | 1-4 workers, auto-terminate, 2 jobs/day |
| Synapse Spark | $100-120 | Slightly cheaper, less features |
| HDInsight | $300-400 | Always-on cluster required |
| Azure Functions | $10-50 | Only for small files |
| DIY on VMs | $500+ | Must manage everything |

### Interview Talking Point
> "I chose Azure Databricks over Synapse Spark because it's the industry-leading Spark platform with best-in-class performance. While Synapse Spark is cheaper by about 20%, Databricks provides the Photon engine (3-4x faster), superior auto-scaling, and better developer productivity. The auto-termination feature reduces costs by 60-70% compared to always-on clusters. For a government agency handling sensitive data, I wanted the most mature, performant, and secure platform available. Databricks is used by thousands of enterprises for production data processing, and it's fully supported in Azure Government Cloud with Premium tier features like RBAC and audit logs."

---

## 5. Azure Synapse Analytics

### What We Used
**Azure Synapse Analytics** with Serverless SQL Pools + Spark Pools

### How It's Used in MVP
```
Components Used:
1. Synapse Workspace (control plane)
2. Serverless SQL Pool (on-demand queries)
3. Spark Pool (optional, for backup processing)

Primary Use Case:
- External tables pointing to Gold layer Delta files
- SQL views for business users
- Power BI connections
- Ad-hoc analytics

SQL Queries:
- SELECT * FROM gold.daily_sales_summary
- SELECT * FROM gold.customer_segmentation
- Views: vw_top_categories, vw_high_value_customers

NOT Used (Yet):
- Dedicated SQL Pool (expensive, not needed for MVP)
- Synapse Pipelines (using Data Factory instead)
- Synapse Link (no real-time operational data yet)
```

### Alternatives Considered

| Option | Description | Why NOT Chosen |
|--------|-------------|----------------|
| **Azure SQL Database** | Traditional PaaS SQL database | âŒ Must import/copy data (storage duplication)<br>âŒ Not optimized for analytics at scale<br>âŒ Expensive for TB+ data<br>âŒ No direct Parquet/Delta access |
| **SQL Server on VMs** | Self-managed SQL Server | âŒ Must manage OS, patching, backups<br>âŒ No auto-scaling<br>âŒ More expensive<br>âŒ Higher operational burden |
| **Power BI Direct Query on Storage** | Power BI reads from Data Lake directly | âŒ Poor performance (no indexing)<br>âŒ No SQL access for analysts<br>âŒ No query history/auditing<br>âŒ Limited to Power BI users only |
| **Databricks SQL** | SQL endpoint in Databricks | âœ… **Valid alternative!**<br>âš ï¸ Requires Databricks SQL license<br>âš ï¸ Less familiar to SQL analysts<br>âš ï¸ Not as integrated with Azure Portal |
| **Azure Data Explorer (ADX)** | Fast time-series analytics | âŒ Optimized for logs/telemetry, not business data<br>âŒ Must ingest data (not query in place)<br>âŒ Different query language (KQL)<br>âŒ Overkill for this use case |
| **Direct File Access** | Analysts use Python/R to read files | âŒ Not user-friendly for SQL analysts<br>âŒ No governance<br>âŒ Everyone needs coding skills<br>âŒ No performance optimization |

### Why Synapse Won

âœ… **Serverless SQL**: Pay per query ($5 per TB scanned), no always-on costs
âœ… **External Tables**: Query Parquet/Delta files directly (no data copy)
âœ… **Familiar SQL**: Analysts already know T-SQL
âœ… **Power BI Integration**: Native connector, optimized queries
âœ… **RBAC**: Leverage AAD groups for access control
âœ… **Query Performance**: Caching, result set optimization
âœ… **Unified Workspace**: One place for SQL, Spark, Pipelines
âœ… **Gov Cloud Support**: Fully available in US Gov Virginia
âœ… **No Data Movement**: Queries data in-place on Data Lake

### Synapse Components Decision

#### 1. Serverless SQL Pool âœ… (Used)
**Why**:
- Pay per query (cost-effective for ad-hoc access)
- No cluster management
- Instant availability
- Perfect for Gold layer queries

**Cost**: $5 per TB scanned (Gold layer is small, ~$10-20/month)

#### 2. Dedicated SQL Pool âŒ (Not Used in MVP)
**Why Not**:
- Expensive: $1,200/month minimum (DW100c)
- Overkill for MVP data volume
- Requires data copy (not query-in-place)
- Adds ETL complexity

**When to Add**:
- Data volume > 10TB
- Need sub-second query response
- Concurrent users > 100
- Complex joins on large tables

#### 3. Spark Pool âš ï¸ (Optional Backup)
**Why Limited Use**:
- Databricks is primary Spark platform
- Slower startup than Databricks
- Backup option if Databricks has issues

**Cost**: $0.18/hour per core (auto-pause saves costs)

### Synapse vs Databricks SQL

| Feature | Synapse Serverless | Databricks SQL | Our Choice |
|---------|-------------------|----------------|------------|
| **Query language** | T-SQL (familiar) | SQL (ANSI) | Synapse (familiarity) |
| **Pricing** | $5 per TB scanned | $0.22 per DBU | Synapse (cheaper) |
| **Startup time** | Instant | 1-2 minutes | Synapse |
| **Power BI integration** | Native | Via connector | Synapse |
| **Azure Portal integration** | Excellent | Limited | Synapse |
| **Delta Lake support** | âœ… Yes | âœ… Yes (better) | Tie |
| **Performance** | Good | Excellent | Databricks |
| **Caching** | Yes | Yes (better) | Databricks |

**Decision**: Use Synapse for SQL queries (cheaper, familiar), Databricks for processing

### Trade-offs Accepted

| Trade-off | Impact | Mitigation |
|-----------|--------|------------|
| Serverless has query limits (30 min timeout) | Can't run extremely long queries | Use Spark for large scans |
| No indexing on external tables | Slower than dedicated SQL pool | Partition data properly, use Delta stats |
| Pay per scan (can get expensive if inefficient queries) | Cost risk | Educate analysts on partition pruning |
| External tables require manual schema updates | Schema drift risk | Automate with CI/CD |

### Cost Comparison (Monthly for 100 Analysts)

| Option | Cost | Notes |
|--------|------|-------|
| **Synapse Serverless** | **$20-50** | Pay per query, most cost-effective |
| Synapse Dedicated (DW100c) | $1,200 | Always-on, better performance |
| Synapse Dedicated (DW500c) | $6,000 | Production scale |
| Azure SQL Database (P15) | $7,200 | Not designed for this |
| Databricks SQL | $200-400 | Pay per DBU, competitive |
| Direct file access (free) | $0 | No SQL layer, poor UX |

### Interview Talking Point
> "I chose Azure Synapse Analytics with Serverless SQL Pools because it provides SQL access to our data lake without copying data or provisioning infrastructure. Unlike Azure SQL Database which requires importing data, Synapse queries Parquet and Delta files directly using external tables. At $5 per TB scanned, it's extremely cost-effective for our 100 analyst users doing ad-hoc queries. The T-SQL interface is familiar to our SQL analysts, and it integrates natively with Power BI. We avoided Dedicated SQL Pools ($1,200+/month minimum) since our MVP data volume doesn't justify the cost. For heavy processing, we use Databricks; for querying, we use Synapseâ€”best tool for each job."

---

## 6. Azure Data Factory

### What We Used
**Azure Data Factory v2** with Pipeline orchestration and Managed Identity authentication

### How It's Used in MVP
```
Primary Functions:
1. Pipeline Orchestration
   - Schedule: Daily at 2 AM
   - Activities: GetMetadata â†’ Databricks â†’ Databricks
   - Monitors: Success/failure notifications

2. Linked Services
   - Storage (via Managed Identity)
   - Databricks (via MSI)
   - Key Vault (for secrets)
   - Synapse (for future integration)

3. Integration Runtime
   - Auto-Resolve IR in US Gov Virginia
   - Handles data movement and activity dispatch

NOT Used:
- Data Flows (using Databricks for transformations)
- Copy Activity (no cross-system copying needed)
- Mapping Data Flows (Databricks handles this)
```

### Alternatives Considered

| Option | Description | Why NOT Chosen |
|--------|-------------|----------------|
| **Azure Synapse Pipelines** | Built-in orchestration in Synapse | âš ï¸ **Very similar to ADF**<br>âŒ Same limitations as ADF<br>âŒ Less mature than ADF<br>âœ… Use if heavily invested in Synapse |
| **Databricks Jobs** | Native job scheduler in Databricks | âŒ No GetMetadata or file checking<br>âŒ Can't orchestrate non-Databricks tasks<br>âŒ Less monitoring/alerting<br>âœ… Good for Databricks-only workflows |
| **Apache Airflow** | Open-source workflow orchestration | âŒ Must self-host (on AKS or VMs)<br>âŒ More operational overhead<br>âŒ Requires Python DAG coding<br>âœ… Better for complex dependencies<br>âœ… More flexible, but more work |
| **Azure Logic Apps** | Low-code workflow automation | âŒ Not designed for data pipelines<br>âŒ Expensive for scheduled runs<br>âŒ Limited data integration<br>âœ… Good for app integration |
| **Azure Functions** | Serverless function execution | âŒ Must write orchestration logic yourself<br>âŒ No visual pipeline editor<br>âŒ No built-in monitoring<br>âœ… Good for custom triggers |
| **Cron Jobs on VMs** | Traditional scheduled scripts | âŒ Must manage infrastructure<br>âŒ No built-in monitoring<br>âŒ No visual pipeline<br>âŒ Not cloud-native |
| **Databricks Workflows** | New orchestration in Databricks (2023+) | âœ… **Emerging option**<br>âš ï¸ Still maturing<br>âš ï¸ Limited Gov Cloud support<br>âœ… Consider for future |

### Why Data Factory Won

âœ… **Native Azure Orchestration**: Purpose-built for Azure services
âœ… **Visual Pipeline Designer**: No-code/low-code pipeline building
âœ… **Managed Service**: No infrastructure to maintain
âœ… **Integration Runtime**: Handles data movement across networks
âœ… **Monitoring**: Built-in pipeline run history and alerting
âœ… **CI/CD Ready**: ARM templates, Git integration, Terraform support
âœ… **Triggers**: Time-based, event-based, tumbling window
âœ… **Cost-Effective**: $1 per 1000 activity runs
âœ… **Gov Cloud**: Fully supported in US Gov Virginia
âœ… **Managed Identity**: No credential management

### Data Factory vs Synapse Pipelines

| Feature | Data Factory | Synapse Pipelines | Our Choice |
|---------|--------------|-------------------|------------|
| **Capabilities** | Full orchestration | Same (copy of ADF) | Tie |
| **UI** | Azure Portal + ADF Studio | Synapse Studio | Slight preference for Synapse |
| **Maturity** | Very mature (2015+) | Newer (2020+) | Data Factory |
| **Gov Cloud support** | Full support | Full support | Tie |
| **Integration with Databricks** | First-class | First-class | Tie |
| **Standalone vs bundled** | Standalone | Part of Synapse | Depends on architecture |
| **Pricing** | Pay per activity | Same | Tie |
| **Git integration** | âœ… Yes | âœ… Yes | Tie |

**Decision**: Chose Data Factory because:
1. More mature and battle-tested
2. Standalone service (not tied to Synapse)
3. We're not heavily using Synapse Pipelines/Spark
4. ADF is the Microsoft recommendation for orchestration

### When to Use Synapse Pipelines Instead
âœ… Already using Synapse SQL/Spark heavily
âœ… Want everything in one workspace
âœ… Don't need standalone orchestration service
âœ… Prefer unified Synapse Studio UI

### Data Factory vs Apache Airflow

| Feature | Data Factory | Airflow | Our Choice |
|---------|--------------|---------|------------|
| **Setup** | Instant (managed) | Must deploy to AKS/VMs | Data Factory |
| **Maintenance** | Zero (Microsoft manages) | Must patch, upgrade | Data Factory |
| **Flexibility** | Limited to ADF activities | Unlimited (Python code) | Airflow (but not needed) |
| **Complex dependencies** | Basic | Advanced (DAGs) | Airflow |
| **Cost** | ~$100/month | ~$500/month (AKS) | Data Factory |
| **Visual editor** | Yes | Limited | Data Factory |
| **Learning curve** | Low | High | Data Factory |
| **Enterprise features** | Built-in | Must configure | Data Factory |

**When Airflow is Better**:
- Need complex conditional logic
- Multi-cloud orchestration
- Heavy Python customization
- Already have Airflow expertise

**Our Case**: Data Factory sufficient (simple linear pipeline)

### Trade-offs Accepted

| Trade-off | Impact | Mitigation |
|-----------|--------|------------|
| Limited to ADF activities | Can't run arbitrary code | Use Databricks for custom logic |
| No complex conditionals | Hard to build decision trees | Keep pipeline simple |
| Debugging can be slow | Must run pipeline to test | Use Databricks notebook dev for logic |
| Vendor lock-in to Microsoft | Hard to switch to other clouds | Use standard formats (Delta, Parquet) |

### Cost Breakdown

**Monthly Cost (Dev Environment)**:
- Pipeline runs: 60 runs/month Ã— $1/1000 = $0.06
- Activity runs: 180 activities/month Ã— $0.001 = $0.18
- Data movement: None (everything in same region)
- **Total: ~$0.24/month** âœ… (virtually free)

**Production Cost (Higher Volume)**:
- Pipeline runs: 3,000 runs/month = $3
- Activity runs: 10,000 activities = $10
- **Total: ~$13/month** (still very cheap)

### Interview Talking Point
> "I chose Azure Data Factory for orchestration because it's a fully managed, cost-effective, and native Azure service. At $1 per 1,000 pipeline runs, it costs virtually nothing for our daily schedule. Unlike Apache Airflow which requires managing infrastructure on AKS and costs $500+/month, Data Factory is instant-on with zero maintenance. The visual pipeline designer makes it easy to understand the workflow at a glance, and it integrates seamlessly with Databricks via managed identityâ€”no credentials to manage. For simple orchestration needs like 'check for files, run notebook, run another notebook,' Data Factory is perfect. If we needed complex conditional logic or multi-cloud orchestration, we'd consider Airflow, but ADF meets our current needs efficiently."

---

## Summary Comparison Table

### Quick Reference: Why Each Service Was Chosen

| Service | What We Used | Key Alternatives | Why We Chose This | Cost (Monthly MVP) |
|---------|--------------|------------------|-------------------|-------------------|
| **Identity** | Azure AD Groups | Individual RBAC, SQL Auth, SAS | Native, scalable, compliant | Included |
| **Storage** | ADLS Gen2 | Blob, Azure Files, SQL DB | Big data optimized, $18/TB | $20 |
| **Architecture** | Medallion (3 layers) | Single container, 2-layer | Industry standard, RBAC | Same |
| **Processing** | Databricks Premium | Synapse Spark, HDInsight | Best performance, mature | $150-200 |
| **Analytics** | Synapse Serverless | SQL Database, Databricks SQL | Pay-per-query, T-SQL | $20-50 |
| **Orchestration** | Data Factory | Synapse Pipelines, Airflow | Managed, cheap, easy | ~$1 |
| **TOTAL** | **~$200-300/month** | | Best value for production-ready platform | |

### Decision Matrix by Priority

#### If Priority is **COST** â¬‡ï¸
1. âœ… Data Factory (pennies)
2. âœ… ADLS Gen2 ($18/TB)
3. âœ… Synapse Serverless ($5/TB queried)
4. âš ï¸ Databricks (most expensive, but essential)

**Could Reduce Costs By**:
- Switch to Synapse Spark (-30%, lose features)
- Use Storage Archive tier for old data (-80%)
- Reduce Databricks cluster size

#### If Priority is **PERFORMANCE** â¬†ï¸
1. âœ… Databricks (Photon engine, 3x faster)
2. âœ… ADLS Gen2 (optimized for big data)
3. âœ… Delta Lake (ACID, Z-order)
4. âš ï¸ Consider Synapse Dedicated SQL for very fast queries

#### If Priority is **COMPLIANCE** ğŸ”’
1. âœ… Azure AD Groups (required for FedRAMP)
2. âœ… Private endpoints (Gov Cloud requirement)
3. âœ… Managed identities (no credential leaks)
4. âœ… All services Gov Cloud supported

#### If Priority is **EASE OF USE** ğŸ‘¥
1. âœ… Data Factory (visual editor)
2. âœ… Synapse (familiar T-SQL)
3. âœ… Databricks (interactive notebooks)
4. âœ… All have good documentation

#### If Priority is **SCALABILITY** ğŸ“ˆ
1. âœ… ADLS Gen2 (petabyte scale)
2. âœ… Databricks (auto-scale to 1000+ cores)
3. âœ… Synapse Serverless (no limits)
4. âœ… All are "infinite scale"

---

## When to Reconsider These Choices

### Signals to Switch from Databricks to Synapse Spark
- Budget cut by >50%
- Data volume drops significantly
- Simple transformations only (no complex logic)
- Heavily invested in Synapse ecosystem

### Signals to Add Synapse Dedicated SQL Pool
- Data volume > 10TB
- Query response time requirements < 1 second
- Concurrent users > 100
- Complex multi-table joins on every query
- Willing to pay $1,200+/month

### Signals to Switch from Medallion to 2-Layer
- Simple use case (no intermediate validation needed)
- Storage costs become significant (>$10K/month)
- No compliance requirements for raw data retention
- Team too small to maintain 3 layers

### Signals to Consider Apache Airflow
- Complex dependencies (10+ conditional paths)
- Multi-cloud orchestration (Azure + AWS)
- Need to run arbitrary Python code in orchestration
- Have dedicated platform engineering team
- Already using Airflow elsewhere

---

## Interview Questions You Can Now Answer

### Q: "Why not use Azure SQL Database for storage?"
**A**: Azure SQL Database costs ~$100/TB/month for Premium tiers, while ADLS Gen2 is $18/TB/month. SQL DB is designed for transactional workloads, not analytics. We'd have to import data (storage duplication), and it doesn't support direct Parquet/Delta access. For analytics, data lake storage is 5x cheaper and better integrated with Spark.

### Q: "Why not use Synapse Spark instead of Databricks?"
**A**: Both are valid choices. I chose Databricks because it's 20-30% faster with the Photon engine, has smoother auto-scaling, and is more mature. Synapse Spark is cheaper (no DBU cost), but Databricks' superior performance and developer experience justify the premium for our use case. If budget was extremely tight, Synapse Spark would be the fallback.

### Q: "Why not use a single container instead of Bronze/Silver/Gold?"
**A**: Separate containers enable different RBAC policiesâ€”analysts only access Gold, engineers access all layers. It also provides clear separation for compliance, allows us to apply different lifecycle policies per layer, and makes data lineage explicit. The storage cost of 3x duplication is offset by lifecycle policies moving old data to Cool tier.

### Q: "Why not use Apache Airflow for orchestration?"
**A**: Airflow is more flexible but requires managing infrastructure on AKS ($500+/month) plus operational overhead. Data Factory is fully managed, costs pennies ($1/month), and has a visual editor. For our simple linear pipeline, ADF is sufficient. If we needed complex conditional logic or multi-cloud orchestration, Airflow would be worth the investment.

### Q: "Why Synapse Serverless instead of Dedicated SQL Pool?"
**A**: Serverless costs $5 per TB scanned with zero infrastructure. Dedicated Pool costs $1,200+/month minimum. Our MVP has <1TB of Gold data with ~100 queries/day, so serverless costs ~$20/month vs $1,200+. We'd switch to Dedicated Pool if data volume exceeds 10TB or if sub-second query response is required.

---

## Summary: The "Goldilocks" Approach

Each service was chosen to be **"just right"** for the MVP:

âŒ **Not Too Simple**: Single VM with Spark = cheaper but not scalable/manageable
âŒ **Not Too Complex**: Self-hosted Kubernetes with Airflow = flexible but overkill
âœ… **Just Right**: Managed services with auto-scaling, pay-per-use pricing

The architecture balances:
- **Cost** (~$200-300/month for dev)
- **Performance** (production-ready speed)
- **Compliance** (FedRAMP High ready)
- **Scalability** (can handle 10-100x growth)
- **Maintainability** (small team can manage)

---

## Final Recommendation

**For MVP**: Keep all current choices âœ…

**For Production**: Consider adding:
1. Synapse Dedicated SQL Pool (if >100 concurrent users)
2. Azure Purview (data catalog)
3. Azure Databricks Unity Catalog (centralized governance)
4. Multi-region deployment (DR)
5. Azure Monitor workbooks (better dashboards)

**For Cost Optimization**:
1. Switch to Synapse Spark if budget <50% current
2. Use storage lifecycle policies aggressively
3. Right-size Databricks clusters based on metrics
4. Use spot instances where possible (future)
